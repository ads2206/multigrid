%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%	Avi Schwarzschild and Andres Soto
%	APMA 4301: Numerical Methods for PDEs 
% Final Project: Multigrid
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[pdftex,12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}		% packages that allow mathematical formatting
\usepackage{graphicx}		% package that allows you to include graphics
\usepackage{setspace}		% package that allows you to change spacing
%\onehalfspacing				% text become 1.5 spaced
\usepackage{fullpage}		% package that specifies normal margins
\addtolength{\topmargin}{-.25 in}
\usepackage{fancyhdr}
\usepackage[table]{xcolor}
\usepackage{amsmath}
\pagestyle{fancy}
\setlength{\headsep}{.5in}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\graphicspath{ {images/} }
	
%-- Begin the body below ---%

\begin{document}
\lhead{Multigrid, \today}
\rhead{Schwarzschild and Soto \thepage}

\input{./title.tex}
\vspace{2 cm}

\begin{abstract}
    We used both and one and two dimenssional poisson problems to study multigrid methods for solving partial differential equations. Using iterative solvers for linear systems we show how coarsening the descritization can lead to approximations which converge to the true solution of the PDE with fewer iterations of the solver. 
\end{abstract}
%-- Sections correspond to problem numbers --%

\section{Motivation}
   
    \paragraph*{} The motivation for using coarser grids, while convergences analysis shows that finer grids should lead to more accurate approximations, comes from an observation about waves, descritizations, and aliasing. The iterative methods, often referred to as smoothers, smooth out error such that in the earlier iterations it is the high frequency components of the error that vanish first. As the algorithms sweep more times the error gets smoother, containing lower frequencies and tending toward zero. The trouble with the classical iterative methods like this is that low frequencies in the error can take many iterations to smooth. In general these iterative methods are $\mathcal{O}(n^2)$. Thus, some improvement in speed is desired.

    \paragraph*{} From the Shannon Sampling Theorem we know that to retain all of the wave information, we need the descritization to have just over two points per wavelength.\footnote{C. E. Shannon, ``Communications in the presence of noise'', Proc. IRE, vol. 37, pp. 10-21, Jan. 1949.} The implication of this on our work is that the highest frequency in the error is determined by the mesh grid. Knowing this, we can coarsen the grid so that we have fewer smaple points of the error function and then the low frequencies with be among the higher ones still contained in the coarse-grid error. By smoothing the error on this coarser grid we can eliminate more componenets of the error than we could without the coarsening. We leave the explanation and implimentation of this for later sections.

\section{Multigrid Method}

notes: 
derive residual equation, and the role of the residual equations



\paragraph{} In this section, we will consider the theory and analyze why multigrid methods work so well.  We will delve deep into gaining some understanding of the linear algebraic implications of mesh restriction and interpolation.  The standard cycle of multigrids is the standard V cycle which looks like: 

[insert diagram of a V cycle]

Transitions from one discretization to another discretization require a mapping of some kind from the current approximation of the true solution or vector to the same vector on the next mesh.  

We will adopt the same notation that Briggs introduces. $\Omega^{h}$ represents a grid with spacing $h = \Delta x $, and naturally $\Omega^{2h}$ represents a coarser grid.   $\mathbf{v}^h$ is an approximation of the true solution $\mathbf{u}^h$, and therefore error is $\mathbf{e}^h = \mathbf{u}^h - \mathbf{v}^h$.  The interpolation matrix maps a vector on a coarse mesh, say $\Omega^{2h}$ to $\Omega^{h}$ and is denoted $I_{2h}^h$, while the restriction matrix maps a vector from $\Omega^{h} \rightarrow \Omega^{2h}$ and is denoted by $R_h^{2h}$.  

Assuming, we have $N+1$ total points in our boundary, and $N-1$ interior points, then the restriction matrix $R_h^{2h}$ maps $\mathbb{R}^{N-1} \rightarrow \mathbb{R}^{N/2 -1}$.  Since we have $N-1$ unknown values and are mapping to a lower dimension subspace, then the rank of our restriction matrix is $\frac{N}{2} -1 $ and the null space of the restriction matrix is $\frac{N}{2}$.  Let us explore how this matrix transforms the eigenvalues of our iterative matrix $A^h$ on mesh $\Omega^h$.  The eigenvectors or modes of $A^h$ are given by 
$$\mathbf{w}_{k}^h = \left( \sin\left( \frac{k \pi}{N}\right), \dots, \sin\left( \frac{(N-1)k \pi}{N}\right) \right)$$ where the jth component of the vector is $w_{k,j}^h = \sin\left( \frac{jk \pi}{N}\right)$.  When the restriction operator is applied directly to these vectors we get: 

\begin{equation}
R_h^{2h} \mathbf{w}_k^h = \frac 1 4 \begin{bmatrix}1 & 2 & 1 \\ & 1 & 2 & 1 \\ & & \ddots & \ddots & \ddots \\ & & & 1 & 2 & 1 \end{bmatrix} \mathbf{w}_k^h  
\end{equation}

The jth component of this is:  
\begin{equation}
\Rightarrow \left( R_h^{2h} \mathbf{w}_k^h \right)_j = 
\frac 1 4 \left(\sin\left(\frac{(j-1)k \pi}{N} \right) + 2 \sin\left(\frac{jk \pi}{N} \right)  + \sin\left(\frac{(j+1) k \pi}{N} \right)  \right)
\end{equation}
Using the following two trigonmetric identities, $\sin(\theta_1 + \theta_2) = \sin \theta_1 \cos\theta_2 + \cos\theta_1\sin\theta_2$ and $\cos(2 \theta) = 2 \cos \theta^2 -1$, leads us to: 
$$
\frac 1 4 \left[ 2 \sin\left(\frac{2jk\pi}{N} \right) \cos\left( \frac{k\pi}{N}\right)  + 2 \sin \left( \frac{2jk \pi}{N} \right) \right] =  \sin \left( \frac{jk \pi }{N / 2}\right) \cos^2 \left( \frac{k \pi}{2N} \right) \text{ for } 1 \leq k \leq \frac N 2 $$

What we find is that this result holds for the first $k/2$ eigenvectors or one half of the unknowns.  
\begin{equation}
R_h^{2h} \mathbf{w}_k^h = \cos^2 \left( \frac{k \pi}{2N}\right) \mathbf{w}_k^{2h}, \, \,  1 \leq k \leq \frac N 2 
\end{equation}

The restriction matrix produces a new kth mode of $A^{2h}$ such that the old kth mode of $A^h$ gets multiplied by a constant.  

Applying a similar analysis for when $ \frac N 2 \leq k \leq N -1$, yields

$$
R_h^{2h} \mathbf{w}_{k'}^h = - \sin^2 \left( \frac{k \pi}{2N} \right) \mathbf{w}_k^{2h}, ~~ 1 \leq k \leq \frac N 2 \text{ where } k' = N-ks
$$ 

Essentially, what is going on here is the phenomena of aliasing since the restriction matrix is acting on the $(N-k)$th mode of $A^h$ and multiplying it by a constant multiple of the .  The interpretation behind this result is that the oscillatory modes on $\Omega^h$ cannot be represented in the coarser mesh $\Omega^{2h}$ and the restriction operator transforms these nodes into relatively smooth modes on the coarser mesh.  The reason this matrix has no inverse is because there are two eigenvectors (eigenvector $k$ and eigenvector $N-k$) on $A^h$ that get mapped to the same kth mode on $A^{2h}$.

INCLUDE SOME GRAPHS HERE TO DESCRIBE THIS ALIASING

We define a set $W_k^h$ that takes the span of these complementary nodes (the $(N-k)$th eigenvector and the $k$th eigenvector), $W_k^h = span \{ \mathbf{w}_k^h, \mathbf{w}_{N-k}^h\}$.  We note that: 

$$
w_{N-k, j}^h = \sin \left( \frac{j (N - k) \pi}{N} \right) = \sin\left( j\pi - \frac{jk \pi}{N}\right)
$$

$$
\sin(j \pi) \cos \left( \frac{jk \pi}{N} \right) - \cos(j \pi) \sin\left( \frac{jk \pi}{N} \right) =  (-1)^{j+1}\sin\left( \frac{jk \pi}{N} \right) = (-1)^{j+1} w_{k, j}^h
$$


Of course, this two element to one element mapping means that $R_h^{2h}$ contains non-trivial elements in its null space.  Let's take the unit vector $\mathbf{e}_i^h$ on $\Omega^h$, and see how it gets transformed by $R_h^{2h}$.  

\begin{equation}
R_h^{2h} \mathbf{e}_i^h = \begin{bmatrix}1 & 2 & 1 \\ & &  1  & 2 & 1 \\ & & & \ddots &\ddots & \ddots \\ & & & & 1 & 2 & 1 \end{bmatrix} \begin{bmatrix} 0 \\ \vdots \\ 1 \\ \vdots \\ 0 \end{bmatrix}
\end{equation}

We notice that there are two cases due to overlapping ones in the odd columns of $R_h^{2h}$.  If $i$ is odd in $\mathbf{e}_i^h$, then we have a contribution from two rows whereas if $i$ is even, there is only one row contribution from only the component with 2.  We have the following: 

\begin{equation}
R_h^{2h} \mathbf{e}_i^h = 
    \begin{cases}
    \frac 1 4 \left( \mathbf{e}_{ \frac{i-1}{2}}^{2h} + \mathbf{e}_{\frac{i+1}{2}}^{2h} \right) ~~ i \text{ odd }\\

    \frac 1 4 \left( 2 \mathbf{e}_{\frac{i}{2}}^{2h} \right) ~~ i \text{ even }\\
    \end{cases}
\end{equation}

These unit vectors do not form the basis of the null space because this matrix product is not zero.  How about we look at the transformation of these unit vectors under the iterative method first, namely  $R_h^{2h}A^h \mathbf{e}_i^h$.  

\begin{equation}
R_h^{2h}A^h \mathbf{e}_i^h = R_h^{2h} \left( - \mathbf{e}_{i-1}^h + 2 \mathbf{e}_{i}^h - \mathbf{e}_{i+1}^h \right)
\end{equation}

If the $i$ is even then $i-1$ and $i+1$ are odd and we'll get extra terms that do not go to zero, whereas if $i$ is odd, we get: 

\begin{equation}
R_h^{2h}A^h \mathbf{e}_i^h =  -\frac 1 4 \left( 2 \mathbf{e}_{\frac{i-1}{2}}^{2h} \right) + \frac 1 2 \left( \mathbf{e}_{\frac{i-1}{2}}^{2h} + \mathbf{e}_{\frac{i+1}{2}}^{2h} \right) - \frac 1 2 \mathbf{e}_{ \frac{i+1}{2}}^{2h} = 0 
\end{equation}

We see that the vectors $\mathbf{n}_i = A^h \mathbf{e}_i^h$ when $i$ is odd form the basis of the null space of the full weighting operator.  The null space contains both smooth and oscillatory modes of $A^h$ because the vector $A^h \mathbf{e}_i^h$, which is a vector in the range of $A^h$, can be decomposed as a linear combination of the eigenvectors mentioned above.  






include image of pair of complemenatary modes on a certain grid


analysis on interpolation operator here














\section{One Dimensional Problem}
    
    \paragraph*{} For the one dimenssional problem we chose to demonstrate the Poisson problem with Dirichlet boundary conditions as follows:
    
    \begin{equation}
    \begin{aligned}
        \frac{d^2u}{dx^2} &= f(x) ~ , ~~ x \in (a, b), \\
        u(a) &= \alpha, \\
        u(b) &= \beta.
    \end{aligned}
    \end{equation}
    
    The values of $f(x)$, $a$, $b$, $\alpha$, and $\beta$ are specified below for two different examples. \\
    \begin{itemize}
        \item Example \#1: 
            \begin{equation*}
                f(x) = -\sin(x) ~ , ~~ a = 0 ~ , ~~ b = 2 \pi ~ , ~~ \alpha = \beta = 0
            \end{equation*}
        \item Example \#2: 
            \begin{equation*}
                f(x) = e^x ~ , ~~ a = 0 ~ , ~~ b = 1 ~ , ~~ \alpha = 1 ~ , ~~ \beta = e
            \end{equation*}
    \end{itemize}
    The results of our method are presented in the Results section, however we will describe the implementation in the next section first.

    \subsection{Implementation}   

        \paragraph*{} Regarding implementation, choosing an appropriate data structure is essential to achieving a high level efficency otherwise, the benefits of the multigrid method is lost.  Briggs, in \textit{A Multigrid Tutorial}, adopts arrays as his primary data structure for one-dimensional problems.  We adopted the same strategy and in general, any arbitrary grid will contain $2^{n-1}$ interir grid points plus two boundary points for a total of $2^{n}+1$.  The size of the arrays are transformed across different mesh grids.  

    \subsection{Results}
        \paragraph*{}

\section{Two Dimensional Problem}
    
    \subsection{Implementation}   
        \paragraph*{}

    \subsection{Results}
        \paragraph*{}

\section{Error Analysis}
    
    \subsection{Multigrid}   
        \paragraph*{}

    \subsection{Gauss-Seidel}
        \paragraph*{}

    \subsection{Successive Over Relaxation}
        \paragraph*{}

\section{Conclusion}













% \subparagraph{a)} 

% \subparagraph{b)} 

% $$
% \rowcolors{2}{gray!25}{white}
% \begin{tabular}{ r  l || r  l }  
% Day & Temp. & Day & Temp. \\ \hline
% 22 & 48.6544 & 91 & 32.6699 \\

% 80 & 29.9413 &  92 & 32.7286 \\

% 81 & 30.3790 & 224 & 80.9063 \\

% 82 & 30.7736 & 225 & 81.5233 \\

% 83 & 31.1268 & 226 & 82.1234 \\

% 84 & 31.4403 & 228 & 83.2654 \\

% 85 & 31.7158 & 229 & 83.8043 \\

% 86 & 31.9551 & 230 & 84.3201 \\

% 87 & 32.1600 & 231 & 84.8117 \\

% 88 & 32.3322 & 232 & 85.2780 \\

% 89 & 32.4734 & 233 & 85.7178 \\

% 90 & 32.5854 & 300 & 88.8952 \\ \hline
% \end{tabular}
% $$

% %-- Next problem: --%
% \section*{}

% \paragraph{Part 2:} 

% \subparagraph{a)} 
% $$
% \rowcolors{2}{gray!25}{white}
% \begin{tabular}{ r c c || r c c }  
% Day & Temp. & Error($\pm$) & Day & Temp. & Error($\pm$) \\ \hline
% 22 & 48.6544 & 0.0093 & 91 & 32.6699 & 22.4093 \\

% 80 & 29.9413 & 22.4093 &  92 & 32.7286 & 22.4093 \\

% 81 & 30.3790 & 22.4093 & 224 & 80.9063 & 0.1493 \\

% 82 & 30.7736 & 22.4093 & 225 & 81.5233 & 0.1493 \\

% 83 & 31.1268 & 22.4093 & 226 & 82.1234 & 0.1493 \\

% 84 & 31.4403 & 22.4093 & 228 & 83.2654 & 1.4006 \\

% 85 & 31.7158 & 22.4093 & 229 & 83.8043 & 1.4006 \\

% 86 & 31.9551 & 22.4093 & 230 & 84.3201 & 1.4006 \\

% 87 & 32.1600 & 22.4093 & 231 & 84.8117 & 1.4006 \\

% 88 & 32.3322 & 22.4093 & 232 & 85.2780 & 1.4006 \\

% 89 & 32.4734 & 22.4093 & 233 & 85.7178 & 1.4006 \\

% 90 & 32.5854 & 22.4093 & 300 & 88.8952 & 0.0093 \\ \hline
% \end{tabular}
% $$

% \section*{}

% \paragraph{Part 3:} 

% \subparagraph{a)} 

% \subparagraph{b)} hey \\ \\


% \section*{}

% \paragraph{Part 4:} 

% \subparagraph{a)} 
% 1. \\ \\

% \subparagraph{b)} Show \emph{quantitatively} why you expect your new choice of interpolation scheme to be an improvement on the original one. \\ \\
% The error estimates for the two interpolation schemes provide us with quantitative reason to think Hermite interpolation has an advantage:

% \subparagraph{c)} 

% $$
% \rowcolors{2}{gray!25}{white}
% \begin{tabular}{ r c c || r c c }  
% Day & Temp. & Error($\pm$) & Day & Temp. & Error($\pm$) \\ \hline
%    22 & 48.6545 & 0.0001 & 91 & 32.6703 & 0.0590 \\

%    80 & 29.9417 & 0.0808 & 92 & 32.7288 & 0.0115 \\

%    81 & 30.3798 & 0.2558 & 224 & 80.9063 & 0.0031 \\

%    82 & 30.7748 & 0.4465 & 225 & 81.5233 & 0.0049 \\

%    83 & 31.1283 & 0.6013 & 226 & 82.1233 & 0.0025 \\

%    84 & 31.4420 & 0.6919 & 228 & 83.2659 & 0.0086 \\

%    85 & 31.7177 & 0.7085 & 229 & 83.8053 & 0.0205 \\

%    86 & 31.9570 & 0.6563 & 230 & 84.3214 & 0.0246 \\

%    87 & 32.1617 & 0.6563 & 231 & 84.8130 & 0.0197 \\

%    88 & 32.3337 & 0.4151 & 232 & 85.2789 & 0.0103 \\

%    89 & 32.4746 & 0.2733 & 233 & 85.7182 & 0.0025 \\

%    90 & 32.5862 & 0.1488 & 300 & 88.8952 & 0.0001 \\ \hline
% \end{tabular}
% $$

% \section*{}

% \paragraph{Part 5:} 

% \subparagraph{a)} 

% \subparagraph{b)} 


% \section*{}

% \paragraph{Part 6:} 

% \subparagraph{a)} 

% \pagebreak

\section{Other}  


Let us assume that the error produced by lies in the column space or range of $I_{2h}^h$, then this means that there exists a vector $\mathbf{u}^{h} \in \Omega^{2h}$ that gets interpolated to $\mathbf{e}^h$ i.e $\mathbf{e}^h = I_{2h}^h \mathbf{u}^{h}$.  Because our iterative method is associated with some matrix $A^h$ on $\Omega^h$ and the residual equation, we find that:

$$A^h \mathbf{e}^h= \mathbf{r}^h = A^h I^h_{2h} \mathbf{u}^{2h}$$

\end{document}